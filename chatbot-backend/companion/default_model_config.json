{
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "temperature": 0.5,
    "max_tokens": 120,
    "top_p": 0.7,
    "stream": true
}
